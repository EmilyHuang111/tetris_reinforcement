{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to train an AI model capable of playing Tetris autonomously for as long as possible. The project will explore various machine learning approaches, including reinforcement learning techniques such as Q-learning or Deep Q-Networks (DQN). Genetic models may also be tested. The performance of each model will be evaluated to determine the most effective approach for mastering the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no data to retrieve. The Tetris game setup was provided including board, piece, and game information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of gathering data involves allowing the model to interact with the Tetris game environment, where it observes the board state and the current piece state. Based on these inputs, the model decides on an action, such as moving, rotating, or dropping the piece, guided by a balance between exploration and exploitation. During exploration, the model chooses random actions to discover new strategies but during exploitation, the model relies on its learned Q-values to select the action with the highest expected reward. Each decision results in feedback from the environment in the form of rewards. Positive rewards are given for clearing lines while negative penalities are for undesirable actions such as losing the game. The performance of the model is assessed based on the outcomes of its decisions, such as lines cleared, which are used to update and refine the model over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Board Setup**\n",
    "\n",
    "1. Dimensions:\n",
    "   - The board has a fixed width of 10 columns and a height of 20 rows, with an additional buffer of 4 rows for the \"spawn zone\" of pieces.\n",
    "\n",
    "2. Data Structures:\n",
    "   - `board`: A 2D list of booleans (`True` or `False`), representing whether a specific cell is occupied.\n",
    "   - `colors`: A parallel 2D list to `board`, storing the color of each piece occupying a cell.\n",
    "   - `widths`: A list where `widths[i]` tracks the number of filled cells in row `i`.\n",
    "   - `heights`: A list where `heights[i]` tracks the maximum height (row index + 1) of any filled cell in column `i`.\n",
    "\n",
    "3. Behavior:\n",
    "   - Row Clearing: Rows are cleared when all cells in a row are filled. This updates both the `board` and the tracking lists (`widths` and `heights`).\n",
    "   - Drop Height Calculation: Determines the height at which a piece can be dropped in a specific column based on its \"skirt\" (lowest points of the piece).\n",
    "\n",
    "4. Potential Bias:\n",
    "   - Row Bias: Rows near the top are more likely to remain empty due to prioritization of clearing lower rows, potentially under-testing gameplay interactions in upper rows.\n",
    "   - Column Bias: Columns with higher heights may constrain piece placement, especially for wider pieces, affecting overall gameplay outcomes.\n",
    "\n",
    "\n",
    "### **Game Setup**\n",
    "\n",
    "1. Gameplay Modes:\n",
    "   - The game supports multiple AI modes: `greedy`, `genetic`, `mcts`, and a `CUSTOM_AI_MODEL` for experimentation. Each AI has different decision-making strategies.\n",
    "   - Includes a random mode (`RandomChoice_NOT_AI`) for benchmarking and comparison.\n",
    "\n",
    "2. Game Loop:\n",
    "   - The game progresses by:\n",
    "     1. Deciding the best move (position and rotation) for the current piece.\n",
    "     2. Dropping the piece to its calculated position.\n",
    "     3. Clearing rows if conditions are met.\n",
    "   - Ends when the top rows are filled.\n",
    "\n",
    "3. Metrics Tracked:\n",
    "   - Pieces Dropped: Total number of pieces successfully placed on the board.\n",
    "   - Rows Cleared: Total number of rows cleared during the game.\n",
    "\n",
    "4. Potential Bias:\n",
    "   - Spawn Zone: The addition of 4 buffer rows may disproportionately benefit narrow or smaller pieces.\n",
    "\n",
    "\n",
    "### **Piece Setup**\n",
    "\n",
    "1. Shapes:\n",
    "   - The game includes classic Tetris shapes (`Horizontal/Vertical Stick`, `T/pyramid`, `S #1`, `S #2`, `L #1`,`L #2`, `Square`), each represented by tuples of coordinates relative to their origin.\n",
    "   - Pieces have a `body` (list of blocks) and a `skirt` (lowest y-coordinates for each x in the piece's width).\n",
    "\n",
    "2. Rotation:\n",
    "   - Rotations are calculated dynamically using a clockwise transformation:\n",
    "     - `(x, y)` becomes `(width - y, x)` for the rotated piece.Pieces can rotate 0,90,180, or 270 degrees. \n",
    "   - The piece is re-aligned to its leftmost position after rotation.\n",
    "\n",
    "3. Colors:\n",
    "   - Each piece is assigned a fixed color, e.g., `RED` for `I`, `GREEN` for `S`, etc.\n",
    "\n",
    "4. Potential Bias:\n",
    "   - Skirt Height Bias: Pieces with smaller skirts (e.g., `O`) may fit into tighter spaces, potentially outperforming larger or more complex pieces (e.g., `L` or `T`).\n",
    "  \n",
    "### **Algorithms** \n",
    " - Genetic\n",
    " - Greedy\n",
    " - Monte Carlo Tree Search\n",
    " - Random\n",
    "  \n",
    "### **custom_model**\n",
    "\n",
    "- Random Rotation:\n",
    "\n",
    " - The piece is randomly rotated 0 to 3 times using the get_next_rotation() method.  \n",
    " - Random X-Position Selection: The AI randomly selects an x-position on the board to drop the piece. It ensures the x-position is within the playable field.\n",
    "\n",
    " - Validation of Placement:\n",
    "\n",
    " - The AI uses board.drop_height(piece, x) to check where the piece would land for the selected x-position. If the position is valid (i.e., it fits on the board), it locks in that position. If the piece doesn't fit (causes an error/exception), it tries a new random x-position.\n",
    "\n",
    "\n",
    " - Return Best Move: Once a valid position is found, the AI returns the x-coordinate and the final rotated version of the piece.\n",
    "\n",
    "\n",
    "### **Extra info**\n",
    "- The board class contains the code for running the tetris game on including no visual and visual. The main.py file is supposed to call the game class and the game class is supposed to call the custom_model. custom_model will load the trained model and the AI will play tetris autonomously. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Exploration Before the Project\n",
    "Before diving into the implementation, I focused on understanding the structure and behavior of the Tetris game. This gave me a solid foundation to design the state, actions, and rewards for my model.\n",
    "Key Steps I Took:\n",
    "Understanding the Environment:\n",
    "\n",
    "\n",
    "I analyzed the mechanics of Tetris, including board dimensions, how the pieces behave (movement, rotation, and dropping), and the conditions for clearing lines or losing the game. \n",
    "Analyzing the State Representation:\n",
    "\n",
    "\n",
    "I thought about how to represent the game state numerically for my model. I decided to include:\n",
    "The board layout as a grid of 0s and 1s to show empty and filled cells.\n",
    "Information about the current piece type, its position, and rotation state.\n",
    "Identifying the Action Space:\n",
    "\n",
    "\n",
    "I explored the possible actions the agent could take, like moving left, moving right, rotating, or dropping the piece. This helped me define the number of discrete actions available in each state.\n",
    "Defining the Reward System:\n",
    "\n",
    "\n",
    "I brainstormed how to structure rewards to guide the agent:\n",
    "Positive rewards for clearing lines.\n",
    "Penalties for creating gaps or losing the game.\n",
    "I tested different reward configurations conceptually to ensure they aligned with my goal of maximizing performance.\n",
    "\n",
    "2. Data Exploration During the Project\n",
    "Key Steps I Took:\n",
    "Exploring the Environment Outputs:\n",
    "\n",
    "\n",
    "States:\n",
    "I verified that the board states were being represented correctly by visualizing the grid and ensuring that the agent’s actions updated the state as expected.\n",
    "Rewards:\n",
    "I analyzed the rewards for various actions to confirm they aligned with the reward system I had defined. For example, I checked if clearing multiple lines gave higher rewards than clearing a single line.\n",
    "Next States:\n",
    "I made sure the next state produced by the environment accurately reflected the consequences of the chosen action.\n",
    "Exploring Agent Behavior:\n",
    "\n",
    "\n",
    "Action Distribution:\n",
    "I examined how often the agent took each type of action, especially during the transition from exploration (random actions) to exploitation (guided by Q-values).\n",
    "Strategy Development:\n",
    "I tracked how the agent’s behavior evolved over time, looking for signs that it was learning effective strategies, like stacking blocks efficiently or prioritizing line clears.\n",
    "Analyzing Training Metrics:\n",
    "\n",
    "Performance Metrics:\n",
    "I tracked metrics such as scores, lines cleared, and survival time to assess the agent’s progress and identify areas for improvement.\n",
    "Testing the Q-Value Predictions:\n",
    "\n",
    "\n",
    "I explored the Q-values predicted by the model to ensure they made sense. For example, I checked if Q-values for actions leading to line clears were higher than those for creating gaps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Converting Game States to Tensors:**\n",
    "  - Transforms board states and features (ex. lines cleared, holes, bumpiness) into PyTorch tensors because Neural networks require tensor inputs for computation.\n",
    "\n",
    "- **Stacking Tensors:**\n",
    "  - Uses torch.stack to group multiple states or next states into a single tensor. This enables parallel processing for faster Q-value predictions.\n",
    "\n",
    "- **Reshaping Rewards:**\n",
    "  - Converts rewards into tensors and reshapes them using [:, None] to match the target Q-value shape.This ensures compatibility for loss computation.\n",
    "\n",
    "- **State Representation Simplification:**\n",
    "  - Extracts features like holes, bumpiness, and total height instead of using the raw board grid. The reduces input complexity, making learning more efficient. Feeding the entire board grid to the neural network would result in 20*10 = 200 inputs. By extracting 4 features such as lines_cleared, holes, bumpiness, and height, the input dimension is drastically reduced. This speeds up learning and allows the neral network to focus only on the most revelvant information.\n",
    " \n",
    "- **Reward function preparation:**\n",
    "\n",
    "- Rewards are designed to encourage clearing lines and discourage bad moves. \n",
    "\n",
    "- Positive Reward: Given for clearing lines, proportional to the number cleared. By placing pieces on the board, there is also a positive reward. \n",
    "- Penalty: Implicitly discouraged by the height, holes, and bumpiness features in the Q-value computation. If a state has high values for undesirable features (ex. holes, bumpiness, or height), the Q-value for that state-action pair will naturally be lower because these features correlate with poor outcomes in Tetris with fewer lines cleared. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experiment with your choices and describe your findings. Finish by selecting a Model to continue with. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research, I realized that a reinforcement model would perform best for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  \n",
    "\n",
    "#define the QLearning class, which inherits from PyTorch's nn.Module\n",
    "class QLearning(nn.Module):\n",
    "    def __init__(self):\n",
    "        #initialize the parent class\n",
    "        super(QLearning, self).__init__()\n",
    "\n",
    "        #First layer: The input takes in 4 features representing the current state of the Tetris game such as height of the tallest column, number of completed rows, number of holes. The output produce 64 features after applying linear transformation and relu activation. The linear layers learns to map the input features to a higher dimensional space. \n",
    "        self.conv1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n",
    "        \n",
    "        #Second layer: The input is 64 features from the first layer and the output is another set of 64 features. By stacking layers, the network can learn more complex relationships. \n",
    "        self.conv2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
    "        \n",
    "        #Third layer: Linear transformation from 64 inputs to 1 output\n",
    "        self.conv3 = nn.Sequential(nn.Linear(64, 1))\n",
    "\n",
    "        #Initialize weights for all layers\n",
    "        self.initialize_weights()\n",
    "\n",
    "    #defines how input data flows through the network. It processes the input data sequentially through the three layers. \n",
    "    def forward(self, input_data):\n",
    "        #Pass the input through the first layer\n",
    "        input_data = self.conv1(input_data)\n",
    "        \n",
    "        #Pass the result through the second layer\n",
    "        input_data = self.conv2(input_data)\n",
    "        \n",
    "        #Pass the result through the third layer to produce the final output\n",
    "        input_data = self.conv3(input_data)\n",
    "\n",
    "        #Return the final output\n",
    "        return input_data\n",
    "\n",
    "    #Initialize the weights of the network using Xavier initialization\n",
    "    def initialize_weights(self):\n",
    "        for module in self.modules():  #Iterate through all modules in the model\n",
    "            if isinstance(module, nn.Linear):  #Check if the module is a Linear layer\n",
    "                #Apply Xavier uniform initialization to the layer's weights. This ensures that the variance of the weights in the network remain consistent across layers. \n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                #Initialize the layer's biases to zero\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "#This neural network takes the state of the Tetris game as input and outputs the Q-value for a specific action. The AI selects the action with the highest Q-value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, my reinforcement model was performing very badly. But after I enhanced my reward function and adjusted the exploration/exploitation strategy, the model's performance improved significantly. After the first 1-500 epochs, my model typically cleared only 1-2 lines. But after more iterations, my model was greatly improving and was able to identify key relattionships in playng the game. After around 3000 epochs, my model was able to clear around several hundreds to thousands of lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model describe the steps taken to acheve the best results possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from game import Game\n",
    "from train import train, parse_config  \n",
    "from random import uniform, randint\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def perform_hyperparameter_search(opt):\n",
    "    #define hyperparameter search space\n",
    "    param_space = {\n",
    "        \"learning_rate\": (1e-5, 1e-3),   #range for learning rate\n",
    "        \"gamma_factor\": (0.8, 0.99),    #discount factor\n",
    "        \"epsilon_start\": (0.8, 1.0),    #initial exploration rate\n",
    "        \"epsilon_end\": (0.01, 0.1),     #minimum exploration rate\n",
    "        \"batch_size\": (128, 512),       #batch size for training\n",
    "        \"decay_epochs\": (500, 2000)     #epochs for epsilon decay\n",
    "    }\n",
    "\n",
    "    best_score = float(\"-inf\")\n",
    "    best_params = None\n",
    "\n",
    "    print(\"Starting hyperparameter search...\\n\")\n",
    "\n",
    "    #perform randomized search\n",
    "    for i in range(20):  #number of random samples\n",
    "        #randomly sample hyperparameters\n",
    "        params = {\n",
    "            \"learning_rate\": uniform(*param_space[\"learning_rate\"]),\n",
    "            \"gamma_factor\": uniform(*param_space[\"gamma_factor\"]),\n",
    "            \"epsilon_start\": uniform(*param_space[\"epsilon_start\"]),\n",
    "            \"epsilon_end\": uniform(*param_space[\"epsilon_end\"]),\n",
    "            \"batch_size\": randint(*param_space[\"batch_size\"]),\n",
    "            \"decay_epochs\": randint(*param_space[\"decay_epochs\"])\n",
    "        }\n",
    "\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "\n",
    "        #update opt with the sampled hyperparameters\n",
    "        opt.learning_rate = params[\"learning_rate\"]\n",
    "        opt.gamma_factor = params[\"gamma_factor\"]\n",
    "        opt.epsilon_start = params[\"epsilon_start\"]\n",
    "        opt.epsilon_end = params[\"epsilon_end\"]\n",
    "        opt.batch_size = params[\"batch_size\"]\n",
    "        opt.decay_epochs = params[\"decay_epochs\"]\n",
    "\n",
    "        #train the model with the updated opt\n",
    "        try:\n",
    "            train(opt)  #run training with updated parameters\n",
    "            score = extract_final_score_from_logs(opt.log_directory)  #extract score\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            score = None\n",
    "\n",
    "        print(f\"Score for these hyperparameters: {score}\\n\")\n",
    "\n",
    "        #update best parameters if the current configuration performs better\n",
    "        if score is not None and score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "    print(\"Hyperparameter search complete.\\n\")\n",
    "    print(f\"Best score: {best_score}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\\n\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a search to find the best possible parameters and I got slightly better results. With the new parameters, my model was able to clear more lines while playing the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing document provide summery of finding and detail approach taken. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetris\n",
    "The goal of this project is to create a machine learning model that is able to play the game Tetris fully autonomous for as long as possible. \n",
    "\n",
    "First, I explored layout of the Tetris game including the game, board, and piece aspects that were provided. \n",
    " - Board is 10 by 20.\n",
    " - There are 7 variations for the shapes of the pieces.\n",
    " - Pieces can rotate 0, 90, 190, or 270 degrees.\n",
    " - Pieces are dropped from the top of the board and the goal of the game is to last the longest by clearing the most lines.\n",
    " - There are many properties of interest such as the bumpiness, holes, or the height sum.\n",
    "\n",
    "Workflow:\n",
    " - Input: Game state features are transformed into tensors.\n",
    " - Prediction: The neural network predicts Q-values for each action.\n",
    " - Action Selection: The agent uses the Q-values to choose actions.\n",
    " - Training: The network updates its weights using experiences while trying to get more rewards.\n",
    " - Outcome: The network improves its ability to predict Q-values, optimizing gameplay strategy over time.\n",
    "\n",
    "Prepare my data:\n",
    "- Converting game states and features like lines cleared, holes, and bumpiness into Pytorch tensors.\n",
    "- Multiple game states or next states are grouped into a single tensor.\n",
    "- Converting rewards into tensors and reshaped to align with the shape of target Q-values.\n",
    "- Extracting features like holes, bumpiness, and total height instead of using the raw 20 by 10 board grid. \n",
    "\n",
    "\n",
    "Then a tested the Q-learning Reinforcement Model. \n",
    "\n",
    "The Reinforcement Model was performing bad at first but after adjusting the reward function, adjusting the exploration/exploitation strategy, and adjusting the neural network, my model significantly improved and was able to clear a lot of lines. \n",
    "\n",
    "Then, I tuned my model from a randomized search and I got slightly improved results!\n",
    "\n",
    "The final results are several hundreds of lines cleared in a single Tetris game!\n",
    "\n",
    "Overall, these results are very good. \n",
    "\n",
    "Several factors contributing to the good results:\n",
    "- Instead of processing the raw board grid, the model uses extracted features like lines cleared, holes, bumpiness, and total height. This reduces the complexity and focuses the learning on the most relevant aspects of the game.\n",
    "- Stores a diverse set of experiences, allowing the model to learn from past mistakes and successes while breaking correlations between consecutive actions.\n",
    "- The exploration/exploitation strategy ensures the agent explores new actions while gradually exploiting known good actions as training progresses.\n",
    "- Rewards proportional to the number of lines cleared incentivize the agent to prioritize line-clearing moves.\n",
    "- The hyperparameter tuning process identified optimal values for learning rate, batch size, epsilon decay, and discount factor, which maximized model performance.\n",
    "- Running the model for thousands of epochs ensured the network had plenty of opportunity to learn complex relationships in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self sufficient and require only your model parameters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the trained model, you must type: python main.py student\n",
    "\n",
    "game.run() for visuals\n",
    "game.run_no_visual() for no visuals\n",
    "\n",
    "In order to train, you must type: python train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
